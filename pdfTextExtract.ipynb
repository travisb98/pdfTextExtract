{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import pprint\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_project:\n",
    "    def  __init__(self,input_path,output_path,templatepath,quantiles,redaction_search_string,stutter_text_pixel_var,csv_exports,json_exports):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.templatepath = templatepath\n",
    "        self.quantiles = quantiles\n",
    "        self.redaction_search_string = redaction_search_string\n",
    "        self.stutter_text_pixel_var = stutter_text_pixel_var\n",
    "        self.csv_exports = csv_exports\n",
    "        self.json_exports = json_exports\n",
    "\n",
    "\n",
    "\n",
    "    def fix_stutter_text(self,word_dict):\n",
    "        # print(\"fix stutter text\")\n",
    "\n",
    "        keys_to_delete_list=[]\n",
    "\n",
    "        #### loop through word dictionary\n",
    "\n",
    "        for key, value in word_dict.items():\n",
    "            \n",
    "            ### if we already marked the key for deletion, skip this iteration\n",
    "            if key in keys_to_delete_list:\n",
    "                continue\n",
    "\n",
    "\n",
    "            ### get all keys where the word is the same, the coordiantes are close, and the key is not the current key\n",
    "\n",
    "            x0 = value[0]\n",
    "            y0 = value[1]\n",
    "            x1 = value[2]\n",
    "            y1 = value[3]\n",
    "            word = value[4]\n",
    "\n",
    "            cur_dupe_key_list = [k for k,v in word_dict.items()\n",
    "                                if (word == v[4]) and\n",
    "                                (key != k) and\n",
    "                                (abs(x0-v[0])<=self.stutter_text_pixel_var) and\n",
    "                                (abs(y0-v[1])<=self.stutter_text_pixel_var) and                                 \n",
    "                                (abs(x1-v[2])<=self.stutter_text_pixel_var) and\n",
    "                                (abs(y1-v[3])<=self.stutter_text_pixel_var)                                                              \n",
    "                                ]\n",
    "            \n",
    "            ### update the keys to delete list\n",
    "            keys_to_delete_list.extend(cur_dupe_key_list)\n",
    "\n",
    "\n",
    "        #### remove the items from the dictionary if they're present in the keys_to_delete list\n",
    "        for delkey in keys_to_delete_list:\n",
    "            del word_dict[delkey]\n",
    "\n",
    "        return word_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######################################################################################\n",
    "    def get_word_dict(self,page):\n",
    "\n",
    "        words = page.get_text(\"words\")\n",
    "        words ={k:v for k,v in enumerate(words)}\n",
    "\n",
    "        return words\n",
    "    \n",
    "    ######################################################################################\n",
    "    def determine_rows(self,word_dict):\n",
    "\n",
    "        temp_word_dict = word_dict.copy()\n",
    "\n",
    "\n",
    "        #### create a blank all rows list\n",
    "        all_rows_list =[]\n",
    "\n",
    "        ################ while temp_word_dict is not empty\n",
    "\n",
    "        while temp_word_dict:\n",
    "\n",
    "            ####  find the min of y0 in the temp dict\n",
    "            minx0 = min([v[0] for v in temp_word_dict.values()])\n",
    "            # print(miny0)\n",
    "            # print(type(miny0))\n",
    "\n",
    "            ##### create a dictinary of these left most words\n",
    "            left_most_words = {k:v for k,v in temp_word_dict.items() if v[0] == minx0}\n",
    "            ### delete the leftmost words\n",
    "            for delkey in [k for k in left_most_words.keys()]:\n",
    "                del temp_word_dict[delkey]\n",
    "\n",
    "\n",
    "        ##### for each leftmost word:\n",
    "            for k,v in left_most_words.items():\n",
    "\n",
    "\n",
    "                ### create blank cur row dict\n",
    "                cur_row ={}\n",
    "                # add the leftmost word to the current row dict\n",
    "                cur_row[k] = v\n",
    "\n",
    "                #### find the other words in the temp_word_dict that line up with the leftmost word. Allow for a small variance?\n",
    "\n",
    "                ##### y ceiling should be y1-((y1 - y0)/quantiles)\n",
    "                y_ceiling = v[3]-((v[3]-v[1])/self.quantiles)\n",
    "                # print(f\"ceiling:{y_ceiling}\")\n",
    "\n",
    "                ### y floor should be y0 + ((y1-y0)/quantiles)\n",
    "                y_floor = v[1]+((v[3]-v[1])/self.quantiles)\n",
    "                # print(f\"floor:{y_floor}\")\n",
    "\n",
    "                ##### line definition\n",
    "\n",
    "                other_words ={}\n",
    "                for ok, ov in temp_word_dict.items():\n",
    "\n",
    "                    midpoint = (ov[1]+ov[3])/2\n",
    "\n",
    "                    if y_ceiling > midpoint and y_floor < midpoint:\n",
    "\n",
    "                        other_words[ok]=ov\n",
    "\n",
    "\n",
    "                #### add the other words in the row to the cur_row_list(extend)\n",
    "                cur_row.update(other_words)\n",
    "\n",
    "                #### remove the other words from the temp_word_dict\n",
    "\n",
    "                for del_key in other_words.keys():\n",
    "                    del temp_word_dict[del_key]\n",
    "\n",
    "\n",
    "                # convert the current dictionary values to a list\n",
    "                cur_row_list = list(cur_row.values())\n",
    "\n",
    "\n",
    "                # sort the current row list by x0\n",
    "                cur_row_list = sorted(cur_row_list,key = lambda x:x[0])\n",
    "\n",
    "\n",
    "                ### add the current row list to the all rows list\n",
    "\n",
    "                all_rows_list.append(cur_row_list)\n",
    "\n",
    "\n",
    "        ### sort all rows by the max of y0\n",
    "        all_rows_list = sorted(all_rows_list,key=lambda x:max(y[3] for y in x))\n",
    "\n",
    "        return all_rows_list\n",
    "    ######################################################################################\n",
    "    def split_to_columns(self,words_list,col_location_dict):\n",
    "\n",
    "        #### create list of empty dictionaries to export\n",
    "        out_dict ={k:[] for k in col_location_dict.keys()}\n",
    "\n",
    "\n",
    "        for key,value in col_location_dict.items():\n",
    "\n",
    "            for row in words_list:\n",
    "                #### if any words are in range\n",
    "                if any([(value[0]<=word[0]<=value[1]) for word in row]):\n",
    "                    ######### append those raw words\n",
    "                    out_dict[key].append(\" \".join([word[4] for word in row if (value[0]<=word[0]<=value[1])]))\n",
    "                else:\n",
    "                    ### append a null string\n",
    "                    out_dict[key].append(\" \".strip())\n",
    "\n",
    "\n",
    "\n",
    "        #### add the raw string as a column to the out dictionary\n",
    "\n",
    "\n",
    "        out_dict[\"Data\"] = [\" \".join(word[4] for word in row)+\" _\" for row in words_list]\n",
    "\n",
    "\n",
    "        #### sort the out dictionary\n",
    "\n",
    "        out_dict = {k: out_dict[k] for k in sorted(out_dict, key=lambda x: x != \"Data\")}\n",
    "\n",
    "\n",
    "        return out_dict\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    def read_template_delimiters(self,file):\n",
    "\n",
    "        current_template_path = os.path.join(self.templatepath,file)\n",
    "\n",
    "        search_results = \"\"\n",
    "\n",
    "        out_dict = {}\n",
    "\n",
    "        ##### read the template page\n",
    "        with fitz.open(current_template_path) as template_doc:\n",
    "\n",
    "            page = template_doc[0]\n",
    "\n",
    "            search_results = page.search_for(self.redaction_search_string)\n",
    "\n",
    "            # pprint.pprint(search_results)\n",
    "\n",
    "        ### sort the search results\n",
    "        search_results = sorted(search_results, key = lambda rect:(rect.x0,rect.x1))\n",
    "        # pprint.pprint(search_results)\n",
    "\n",
    "\n",
    "        ##### generate dictionary of min and max ranges for each column\n",
    "        prior_cutoff = 0\n",
    "        for index, rectangle in enumerate(search_results):\n",
    "\n",
    "            x0,y0,x1,y1 = rectangle\n",
    "\n",
    "            current_label = \"Data \"+str(index)\n",
    "            current_cutoff = (x1+x0)/2\n",
    "\n",
    "\n",
    "            out_dict.update({current_label:(prior_cutoff,current_cutoff)})\n",
    "\n",
    "\n",
    "            prior_cutoff = current_cutoff\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "\n",
    "\n",
    "    def draw_line_on_page(self,page,template_delimiters):\n",
    "        page_height = page.rect.height\n",
    "        for _, x_coor in template_delimiters.values():\n",
    "            page.draw_line((x_coor, 0), (x_coor, page_height), color=(1, 0, 0),width=0.5)\n",
    "\n",
    "        return page\n",
    "\n",
    "\n",
    "    def file_level_processing(self,current_path):\n",
    "            \n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        df_list =[]\n",
    "        \n",
    "        ##### no extension\n",
    "        file_name =os.path.splitext(os.path.split(current_path)[-1])[0]\n",
    "    \n",
    "\n",
    "\n",
    "        #### with extension\n",
    "\n",
    "        file = os.path.basename(current_path)\n",
    "\n",
    "\n",
    "\n",
    "        ###### get the delimiters\n",
    "\n",
    "        template_delimiters = self.read_template_delimiters(file)\n",
    "\n",
    "        \n",
    "\n",
    "        with fitz.open(current_path) as doc:\n",
    "                \n",
    "            for pagenum, page in tqdm.tqdm(enumerate(doc),total = doc.page_count,desc=file_name):\n",
    "                # print(page.rotation)\n",
    "\n",
    "\n",
    "                #### clean the page do deal with Q wrapping issue\n",
    "                page.wrap_contents()\n",
    "\n",
    "\n",
    "                ### get the word dictionaries\n",
    "                word_dict = self.get_word_dict(page)\n",
    "\n",
    "\n",
    "                ## fix stutter text issue\n",
    "                word_dict = self.fix_stutter_text(word_dict=word_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #### testing(export json dictionary)\n",
    "\n",
    "                if self.json_exports:\n",
    "                    with open(os.path.join(self.output_path,file+\"-\"+str(pagenum)+\".json\"),\"w\") as json_file:\n",
    "                        json.dump(word_dict,json_file, indent=4)\n",
    "                \n",
    "\n",
    "\n",
    "                ##### determine which words belong on which rows\n",
    "                words_list = self.determine_rows(word_dict)\n",
    "\n",
    "                ### split the data into columns\n",
    "                words_dict =self.split_to_columns(words_list,template_delimiters)\n",
    "\n",
    "                #### couvert the dictionary into a data frame\n",
    "                sub_df = pd.DataFrame(words_dict)\n",
    "\n",
    "                ##### convert the data to strings\n",
    "                for colnam in sub_df.columns:\n",
    "                    # print(colnam)\n",
    "                    sub_df[colnam]=sub_df[colnam].astype(str)\n",
    "\n",
    "                #### add the page number and reorder the columns\n",
    "                sub_df[\"PageNum\"]=pagenum+1\n",
    "                new_order = [\"PageNum\"] + [col for col in sub_df.columns if col != \"PageNum\"]\n",
    "                sub_df=sub_df[new_order]\n",
    "\n",
    "\n",
    "\n",
    "                #### add the file name to the df the reorder it\n",
    "                sub_df[\"File\"]=file_name\n",
    "                new_order = ['File'] + [col for col in sub_df.columns if col != 'File']\n",
    "                sub_df=sub_df[new_order]\n",
    "\n",
    "\n",
    "\n",
    "                df_list.append(sub_df)\n",
    "\n",
    "\n",
    "                ###### draw a line on the out put page\n",
    "                page = self.draw_line_on_page(page,template_delimiters)\n",
    "            \n",
    "            doc.save(os.path.join(self.output_path,file))\n",
    "        print(f\"Combining dataframe pages for file: {file_name}\")\n",
    "        df = pd.concat(df_list,ignore_index=True)\n",
    "\n",
    "\n",
    "        if self.csv_exports:\n",
    "            print(f\"Exporting to CSV:{file_name}\")\n",
    "            df.to_csv(os.path.join(self.output_path,file_name+\".csv\"),index_label=\"Index\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "        #### to excel could be really slow for big files\n",
    "            print(f\"Exporting to Excel: {file_name}\")\n",
    "            df.to_excel(os.path.join(self.output_path,file_name+\".xlsx\"),index_label=\"Index\")\n",
    "\n",
    "        return df\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    #### not using this here, importing this function\n",
    "    def peel_first_page_templates(self):\n",
    "\n",
    "        ### make sure the template path exists\n",
    "        if not os.path.exists(self.templatepath):\n",
    "            os.makedirs(self.templatepath)\n",
    "\n",
    "\n",
    "        for file in tqdm.tqdm(os.listdir(self.input_path)):\n",
    "            inpath = os.path.join(self.input_path,file)\n",
    "            outpath = os.path.join(self.templatepath,file)\n",
    "\n",
    "            #### open the file and get the first page\n",
    "\n",
    "            with fitz.open(inpath) as indoc:\n",
    "\n",
    "                outdoc = fitz.open()\n",
    "\n",
    "                outdoc.insert_pdf(indoc,from_page=0,to_page=0)\n",
    "\n",
    "                outdoc.save(outpath)\n",
    "\n",
    "        print(\"Done\")\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        #### make sure the output folder exists\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        df_list =[]\n",
    "\n",
    "\n",
    "        for file in os.listdir(self.input_path):\n",
    "    \n",
    "            current_path = os.path.join(self.input_path,file)\n",
    "            df = self.file_level_processing(current_path)\n",
    "\n",
    "\n",
    "            df_list.append(df)\n",
    "\n",
    "            print(f\"Dataframe Length for {file} : {len(df)}\")\n",
    "            print(\"----------------------------------------------\")\n",
    "\n",
    "        # df = pd.concat(df_list)\n",
    "        # print(f\"main len{len(df)}\")\n",
    "\n",
    "        return df\n",
    "    ######################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PAY F1: 100%|██████████| 106/106 [00:00<00:00, 106.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining dataframe pages for file: PAY F1\n",
      "Exporting to Excel: PAY F1\n",
      "Dataframe Length for PAY F1.pdf : 2159\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "\n",
    "    parent_folder = os.path.abspath('')\n",
    "\n",
    "\n",
    "\n",
    "    #### don't explicitly define these paths, use relative paths to the CWD folder\n",
    "    extraction = extract_project(\n",
    "\n",
    "        \n",
    "\n",
    "        input_path = os.path.join(parent_folder,\"input\"),\n",
    "        output_path = os.path.join(parent_folder,\"output\"),\n",
    "        templatepath = os.path.join(parent_folder,\"template_pages\"),\n",
    "        quantiles=4,\n",
    "        redaction_search_string = \"@\",\n",
    "        stutter_text_pixel_var =1,\n",
    "        csv_exports = False,\n",
    "        json_exports = False\n",
    "\n",
    "    )\n",
    "\n",
    "    try:\n",
    "\n",
    "\n",
    "        extraction.main()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "    # extraction.peel_first_page_templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tflipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
